{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own Transformer-Based LLM From Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook contains a miniature implementation of the famous [\"Attention Is All You Need\" paper](https://arxiv.org/pdf/1706.03762)'s transformer architecture with layers/components built from scratch using PyTorch. This architecture is what started the snowball of high performance LLMs in the recent years, including the GPT models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.1 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (3.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from jinja2->torch==2.3.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from sympy->torch==2.3.1) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PyTorch & Required Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b3eab70>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# set random seed for reproducible results \n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and observe the training data\n",
    "Note the destination directory that the data is being downloaded into! \n",
    "#### We'll be using a small Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  2683k      0 --:--:-- --:--:-- --:--:-- 2682k\n",
      "length of dataset in chars: 1115394\n",
      "first thousand chars: \n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/karpathy/ng-video-lecture/refs/heads/master/input.txt -o input_data\n",
    "\n",
    "with open('input_data', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataset in chars:\", len(text))\n",
    "print(\"first thousand chars: \\n\", text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "# get all unique characters\n",
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "\n",
    "# create tokenizer fpr the input text by characters\n",
    "str2int = { ch:i for i,ch in enumerate(vocab)}\n",
    "int2str = { i:ch for i,ch in enumerate(vocab)}\n",
    "encoder = lambda s: [str2int[c] for c in s] # given string, return list of ints \n",
    "decorder = lambda l: ''.join([int2str[i] for i in l]) # given list, return string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text and prepare test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input dataset and place it in tensor \n",
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "\n",
    "# train validation split \n",
    "n = int(.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Start Building the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Block\n",
    "A feed forward block is a simple neural network layer that consists of two linear transformations, one after the other. This feed forward block contains a ReLU between the two linear layers. The ReLU function, or Rectified Linear Unit, is a mathematical function that returns the input if it is positive, and 0 if it is negative. It is commonly used as an activation function in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" simple linear layer followed by non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * 4), # scale computation by 4\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_embed * 4, n_embed), # scale residual computation by 4\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Head & Multi-Head Attention Block\n",
    "**Attention Head**: An attention head is a sub-component of a multi-head attention block. It takes a query vector, a set of key-value pairs, and outputs a weighted sum of the values based on the similarity between the query and the keys.\n",
    "**Multi-Head Attention Block**: A multi-head attention block is a neural network layer that consists of multiple attention heads. It takes a query vector, a set of key-value pairs, and outputs a weighted sum of the values based on the similarity between the query and the keys. The output of the multi-head attention block is then passed through a feed forward block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    \"\"\" single head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', tensor=torch.tril(torch.ones(block_size, block_size))) # adding as a constant parameter\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x) # B,T,C\n",
    "        q = self.query(x) # B,T,C\n",
    "\n",
    "        # compute the affinities i.e. the attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # B,T,C multiplied by B, C, T produces B, T, T\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # B, T, T\n",
    "        wei = F.softmax(wei, dim=-1) # size is B,T,T\n",
    "        wei = self.dropout(wei)\n",
    "        # perform weighted aggregation of values\n",
    "        v = self.value(x) # B,T,C\n",
    "        weighted_aggregation = wei @ v # B,T,T multiplied by B,T,C produces B,T,C\n",
    "\n",
    "        return weighted_aggregation\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    \"\"\" multiple heads of self attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, block_size, head_size, dropout, n_embed):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, dropout) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(n_heads * head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        output = self.dropout(self.projection(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "A transformer block is a neural network layer that consists of a multi-head attention block and a feed forward block. It takes a query vector, a set of key-value pairs, and outputs a weighted sum of the values based on the similarity between the query and the keys. The output of the transformer block is then passed through a feed forward block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    \"\"\" transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, block_size, head_size, dropout, n_embed)\n",
    "        self.ffwd = FeedForward(n_embed, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together!\n",
    "A BigramLanguageModel is a type of language model that predicts the next word in a sentence based on the previous **two words**, hence the prefix (Bi-). We will build it using the blocks we previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters \n",
    "block_size = 128 # maximum context length for prediction\n",
    "n_embed = 142 # number of embedding dimensions \n",
    "n_heads = 4\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "vocab_size = 65\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed) # this will allow us to encode the meaning of the tokens \n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed) # this will allow us to encode the position of the tokens\n",
    "        # self.sa_heads = MultiHeadAttention(num_heads=4, head_size=n_embed//4)\n",
    "        # self.ffwd = FeedForward(n_embed=n_embed)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embed, n_heads, block_size, dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # finaly layer normalization \n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B,T,C)\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "        x = token_embeddings + position_embedding # (B,T,C)\n",
    "        # x = self.sa_heads(x) # apply one head of self attention\n",
    "        # x = self.ffwd(x) # (B,T,C) - this layer allows the model to 'think' on the context before producing the logits\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None: # in the case where we are running inference\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # convert array to 2Dtr\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, target=targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices for the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # get last time step, (B,C)\n",
    "            probs = F.softmax(logits, dim=1) # get probability \n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how the model performs prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tgRVxeeoJKVchAPRB,$pX!B;wIsUJqKMD.fIYwqlwT.zZh-.!A wyoFMPC&ifW.wXWwAAj3H;ItqA3WusQPmbMsqQpIWkyUT'k.R\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel()\n",
    "print(decorder(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a bunch of completely random tokens from our vocab. This is expected as the model is initialized to random parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 0: 4.3404 \n",
      "Validation loss at step 0: 4.3501\n",
      "Training loss at step 1000: 2.4414 \n",
      "Validation loss at step 1000: 2.4643\n",
      "Training loss at step 2000: 2.2943 \n",
      "Validation loss at step 2000: 2.3042\n",
      "Training loss at step 3000: 2.1522 \n",
      "Validation loss at step 3000: 2.2031\n",
      "Training loss at step 4000: 2.0412 \n",
      "Validation loss at step 4000: 2.1139\n"
     ]
    }
   ],
   "source": [
    "# define training hyperparameters\n",
    "batch_size = 4 # number of independent sequences processed in parallel\n",
    "max_iterations = 5000\n",
    "learning_rate = 3e-4\n",
    "eval_interval = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "\n",
    "# initialize the model we created\n",
    "model = BigramLanguageModel()\n",
    "\n",
    "# define a function that generates batches of data for training or validation. \n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) # generate $batch_size number of random indexes\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # batch chunks for each random index \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # offset by 1 for next char prediction\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() #setting to no back propagation! more efficient memory mode\n",
    "\n",
    "\n",
    "# define function to calculate the mean loss for the model during training and testing \n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval # switch to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # switch back to train mode\n",
    "    return out \n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "\n",
    "# get pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "for iter in range(max_iterations):\n",
    "\n",
    "    # evaluate loss on training and validation data every few iterations \n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Training loss at step {iter}: {losses['train']:.4f} \\nValidation loss at step {iter}: {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample generation!\n",
    "\n",
    "Let's see how the model performs post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whaly prase, agesif! Ines:\n",
      "You wall, knoway your prave lomy me.\n",
      "\n",
      "LOENCENTER:\n",
      "Tholk 'This an delfs!\n",
      "\n",
      "ANGHAH:\n",
      "Thes, as INould haple dam the kinking theing\n",
      "You shiche shall to his intesatool's lordcy gonge!\n",
      "\n",
      "MERCUans, Yorkize, What othr lous I Hey,'d were lancalt it\n",
      "If he reguan, larnd a ongenow: not Lord giefuld:\n",
      "On nir sigisent\n",
      "En Ravent on my pruc des,\n",
      "Foor bage'd booth of ther of berpoble.\n",
      "Dey! fay!\n",
      "To cousis, and tur hee she spirthe hat dank\n",
      "Noble of infur the pludmy dease nedt sugee gring;\n",
      "An\n"
     ]
    }
   ],
   "source": [
    "print(decorder(model.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given our hardware and time restrictions its not great. The larger you make the model's hyperparameters (especially n_embed, which is the number of embeddings we capture), the more likely it is to perform better, but the longer it'll take to train due to the increase in the number of model weights to be computed/optimized. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
