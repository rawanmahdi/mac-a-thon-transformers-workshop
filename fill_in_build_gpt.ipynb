{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own Transformer-Based LLM From Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook contains a miniature implementation of the famous [\"Attention Is All You Need\" paper](https://arxiv.org/pdf/1706.03762)'s transformer architecture with layers/components built from scratch using PyTorch. This architecture is what started the snowball of high performance LLMs in the recent years, including the GPT models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.3.1 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (3.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from torch==2.3.1) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from jinja2->torch==2.3.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/rawanmahdi/Documents/rag-sandbox/.conda/lib/python3.11/site-packages (from sympy->torch==2.3.1) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import PyTorch & Required Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x113f8f170>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# set random seed for reproducible results \n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and observe the training data\n",
    "Note the destination directory that the data is being downloaded into! \n",
    "#### We'll be using a small Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  4247k      0 --:--:-- --:--:-- --:--:-- 4254k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/karpathy/ng-video-lecture/refs/heads/master/input.txt -o input_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique characters\n",
    "\n",
    "# create tokenizer fpr the input text by characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode text and prepare test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode input dataset and place it in tensor \n",
    "\n",
    "# train validation split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Start Building the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Block\n",
    "A feed forward block is a simple neural network layer that consists of two linear transformations, one after the other. This feed forward block contains a ReLU between the two linear layers. The ReLU function, or Rectified Linear Unit, is a mathematical function that returns the input if it is positive, and 0 if it is negative. It is commonly used as an activation function in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" simple linear layer followed by non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * 4), # scale computation by 4\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_embed * 4, n_embed), # scale residual computation by 4\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.net(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Head & Multi-Head Attention Block\n",
    "**Attention Head**: An attention head is a sub-component of a multi-head attention block. It takes a query vector, a set of key-value pairs, and outputs a weighted sum of the values based on the similarity between the query and the keys.\n",
    "**Multi-Head Attention Block**: A multi-head attention block is a neural network layer that consists of multiple attention heads. It takes a query vector, a set of key-value pairs, and outputs a weighted sum of the values based on the similarity between the query and the keys. The output of the multi-head attention block is then passed through a feed forward block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    \"\"\" single head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return weighted_aggregation\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    \"\"\" multiple heads of self attention in parallel \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "A transformer block is a neural network layer that consists of a multi-head attention block and a feed forward block. It takes a query vector, a set of key-value pairs, and outputs a weighted sum of the values based on the similarity between the query and the keys. The output of the transformer block is then passed through a feed forward block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    \"\"\" transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, n_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together!\n",
    "A BigramLanguageModel is a type of language model that predicts the next word in a sentence based on the previous **two words**, hence the prefix (Bi-). We will build it using the blocks we previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters \n",
    "block_size = 128 # maximum context length for prediction\n",
    "n_embed = 142 # number of embedding dimensions \n",
    "n_heads = 4\n",
    "n_layers = 6\n",
    "dropout = 0.2\n",
    "vocab_size = 65\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "       \n",
    "    def forward(self, idx, targets=None):\n",
    "    \n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "     \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how the model performs prior to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tgRVxeeoJKVchAPRB,$pX!B;wIsUJqKMD.fIYwqlwT.zZh-.!A wyoFMPC&ifW.wXWwAAj3H;ItqA3WusQPmbMsqQpIWkyUT'k.R\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel()\n",
    "print(decorder(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a bunch of completely random tokens from our vocab. This is expected as the model is initialized to random parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step 0: 4.3404 \n",
      "Validation loss at step 0: 4.3501\n",
      "Training loss at step 1000: 2.4414 \n",
      "Validation loss at step 1000: 2.4643\n",
      "Training loss at step 2000: 2.2943 \n",
      "Validation loss at step 2000: 2.3042\n",
      "Training loss at step 3000: 2.1522 \n",
      "Validation loss at step 3000: 2.2031\n",
      "Training loss at step 4000: 2.0412 \n",
      "Validation loss at step 4000: 2.1139\n"
     ]
    }
   ],
   "source": [
    "# define training hyperparameters\n",
    "batch_size = 4 # number of independent sequences processed in parallel\n",
    "max_iterations = 5000\n",
    "learning_rate = 3e-4\n",
    "eval_interval = 1000\n",
    "eval_iters = 200\n",
    "\n",
    "\n",
    "# initialize the model we created\n",
    "model = BigramLanguageModel()\n",
    "\n",
    "# define a function that generates batches of data for training or validation. \n",
    "def get_batch(split):\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() #setting to no back propagation! more efficient memory mode\n",
    "\n",
    "\n",
    "# define function to calculate the mean loss for the model during training and testing \n",
    "def estimate_loss():\n",
    "   \n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "\n",
    "# get pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "for iter in range(max_iterations):\n",
    "\n",
    "    # evaluate loss on training and validation data every few iterations \n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Training loss at step {iter}: {losses['train']:.4f} \\nValidation loss at step {iter}: {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample generation!\n",
    "\n",
    "Let's see how the model performs post training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whaly prase, agesif! Ines:\n",
      "You wall, knoway your prave lomy me.\n",
      "\n",
      "LOENCENTER:\n",
      "Tholk 'This an delfs!\n",
      "\n",
      "ANGHAH:\n",
      "Thes, as INould haple dam the kinking theing\n",
      "You shiche shall to his intesatool's lordcy gonge!\n",
      "\n",
      "MERCUans, Yorkize, What othr lous I Hey,'d were lancalt it\n",
      "If he reguan, larnd a ongenow: not Lord giefuld:\n",
      "On nir sigisent\n",
      "En Ravent on my pruc des,\n",
      "Foor bage'd booth of ther of berpoble.\n",
      "Dey! fay!\n",
      "To cousis, and tur hee she spirthe hat dank\n",
      "Noble of infur the pludmy dease nedt sugee gring;\n",
      "An\n"
     ]
    }
   ],
   "source": [
    "print(decorder(model.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given our hardware and time restrictions its not great. The larger you make the model's hyperparameters (especially n_embed, which is the number of embeddings we capture), the more likely it is to perform better, but the longer it'll take to train due to the increase in the number of model weights to be computed/optimized. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
